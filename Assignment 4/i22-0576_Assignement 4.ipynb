{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd097a40",
   "metadata": {
    "id": "fd097a40"
   },
   "source": [
    "# Question1\n",
    " POETRY Generation using N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd8420",
   "metadata": {
    "id": "bbfd8420"
   },
   "source": [
    "1 Introduction:\n",
    "In this assignment, you will use n-gram language modeling to generate some poetry using the ngrams. For the purpose of this assignment a poem will consist of three stanzas each containing four verses where each verse consists of 7—10 words. For example, following is a manually generated stanza.\n",
    "\n",
    "دل سے نکال یاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "ہوتا ہے کیوں اداس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مایوسیوں کی قید سے خود کو نکال کر،\n",
    "\n",
    "آ جاؤ میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "آ کر کبھی تو دید سے سیراب کر مجھے،\n",
    "\n",
    "مرتی نہیں ہے پیاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مہر و وفا خلوص و محبت گداز دل،\n",
    "\n",
    "سب کچھ ہے میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "لوٹیں گے تیرے آتے ہی پھر دن بہار کے،\n",
    "\n",
    "رہتی ہے دل میں آس کہ زندہ ہوں میں،\n",
    "\n",
    "نایا ب شاخ چشم میں کھلتے ہیں اب بھی خواب، سچ ہے ترا\n",
    "\n",
    "قیاس کہ زندہ ہوں میں ابھی\n",
    "\n",
    "The task is to print three such stanzas with an empty line in between. The generation model can be trained on the provided Poetry Corpus containing poems from Faiz, Ghalib and Iqbal.You can scrape other urdu poetry too from internet. You will train unigram and bigram models using this corpus. These models will be used to generate poetry.\n",
    "\n",
    "2 Assignment Task:\n",
    "\n",
    "The task is to generate a poem using different models. We will generate a poem verse by verse until all stanzas have been generated. The poetry generation problem can be solved using the following algorithm:\n",
    "1. Load the Poetry Corpus\n",
    "2. Tokenize the corpus in order to split it into a list of words\n",
    "3. Generate n-gram models\n",
    "4. For each of the stanzas\n",
    "– For each verse\n",
    "* Generate a random number in the range [7...10]\n",
    "* Select first word\n",
    "* Select subsequent words until end of verse\n",
    "* [bonus] If not the first verse, try to rhyme the last word with the last word of the previous verse\n",
    "* Print verse\n",
    "– Print empty line after stanza\n",
    "2.1 Implementation Challenges:\n",
    "\n",
    "Among the challenges of solving this assignment will be selecting subsequent words once we have chosen the first word of the verse. To predict the next word, what we aim to compute is the most probable next word from all the possible next words. In other words, we need to find the set of words that occur most frequently after the already selected word and choose the next word from that set. We can use a Conditional Frequency Distribution (CFD) to figure that out! A CFD tells us: given a condition, what is likelihood of each possible outcome. [bonus] Rhyming the generated verses is also a challenge. You can build your dictionary for rhyming. The Urdu sentence is written from right to left, so makes your n-gram models according to this style.\n",
    "\n",
    "2.2 Standard n-gram Models\n",
    "We can develop our model using the Conditional Frequency Distribution method. First develop a unigram model (Unigram Model), then the bigram model (Bigram Model) and then trigram model. Select the first word of each line randomly from starting words in the vocabulary and then use the bigram model to generate the next word until the verse is complete. Generate the next three lines similarly.\n",
    " Follow the same steps for the trigram model and compare the results of the two n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74e5d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کی کیا تفصیر ہے؟ آب تو دلبری کیا آبرو میں\n",
      "مجھ کو یہ کافر کو بخشی سختی خارا رہے ہیں\n",
      "کہ کبھی ملے یہ حرف شیریں ترجماں تیرا کہیں\n",
      "ہو سب دل نہیں غیر از سیلیِ استاد‘ نہیں\n",
      "\n",
      "کمخواب تھا جن کے بدلے دشنہ مژگاں\n",
      "قبا کا؟ ذرہ سویداۓ بیاباں نورد تھا واں\n",
      "اعتمادِ دل تا پا نہ سکے گی جو واں\n",
      "لیے ورق تمام ہے بے شرف صحبت اے ناداں\n",
      "\n",
      "میرے نصیب کر کوئی موجود پھر مشاہدہ ہے\n",
      "کی گستاخی دے کوئی اداس بیٹھا ہے\n",
      "لگے نہ ہو جس سے گزر گرچہ ہے\n",
      "کام بند کریں دماغ خجلت ہوں گلخن میں‘ ہے\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk import ngrams\n",
    "\n",
    "# Load the Poetry list from iqbal.txt and ghalib.txt\n",
    "poetry_list = []\n",
    "\n",
    "with open('iqbal.txt', 'r', encoding='utf-8') as file:\n",
    "    poetry_list += file.read().splitlines()\n",
    "\n",
    "with open('ghalib.txt', 'r', encoding='utf-8') as file:\n",
    "    poetry_list += file.read().splitlines()\n",
    "\n",
    "# Tokenize the poetry_list into words\n",
    "\n",
    "words = []\n",
    "for line in poetry_list:\n",
    "    word_list = line.split()\n",
    "    for word in word_list:\n",
    "        words.append(word)\n",
    "\n",
    "# Generate Bigram words\n",
    "bigram_words = {}\n",
    "\n",
    "for word1, word2 in ngrams(words, 2):\n",
    "    if word1 not in bigram_words:\n",
    "        bigram_words[word1] = []\n",
    "    bigram_words[word1].append(word2)\n",
    "\n",
    "# Function to generate a ghazal verse using a model\n",
    "def generate_ghazal_verse(model, length_range):\n",
    "    verse = []\n",
    "    length = random.randint(length_range[0], length_range[1])\n",
    "    \n",
    "    while length > 0:\n",
    "        if not verse:\n",
    "            # Start with a random word from the poetry list\n",
    "            current_word = random.choice(words)\n",
    "        else:\n",
    "            # Choose the next word based on the bigram words\n",
    "            next_words = model.get(verse[-1], [])\n",
    "            if not next_words:\n",
    "                break\n",
    "            current_word = random.choice(next_words)\n",
    "        \n",
    "        verse.append(current_word)\n",
    "        length -= 1\n",
    "    \n",
    "    return verse\n",
    "\n",
    "# Function to check if the last word of the verses rhyme\n",
    "def last_words_rhyme(verses):\n",
    "    last_word = verses[0][-1]\n",
    "    for verse in verses[1:]:\n",
    "        if last_word[-2:] != verse[-1][-2:]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Generating full ghazal with rhyming stanza \n",
    "def generate_complete_ghazal(stanza_count, verses_per_stanza):\n",
    "    for _ in range(stanza_count):\n",
    "        while True:\n",
    "            \n",
    "            rhyming_verses = []\n",
    "            for _ in range(verses_per_stanza):\n",
    "                verse = generate_ghazal_verse(bigram_words, (7, 10))\n",
    "                rhyming_verses.append(verse)\n",
    "            \n",
    "            if last_words_rhyme(rhyming_verses):\n",
    "                break\n",
    "        \n",
    "        for verse in rhyming_verses:\n",
    "            print(\" \".join(verse))\n",
    "        \n",
    "        print()  \n",
    "\n",
    "generate_complete_ghazal(3, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79601988",
   "metadata": {
    "id": "79601988"
   },
   "source": [
    "# Question2\n",
    " Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa286b84",
   "metadata": {
    "id": "fa286b84",
    "outputId": "eed1da3f-fabe-4ef1-ba32-dbf482163b85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bdad25b",
   "metadata": {
    "id": "1bdad25b"
   },
   "outputs": [],
   "source": [
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266654b6",
   "metadata": {
    "id": "266654b6",
    "outputId": "38cd33e4-19c0-4338-a6af-6b2951888fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arabic': 0, 'azerbaijani': 3, 'basque': 0, 'bengali': 0, 'catalan': 3, 'chinese': 0, 'danish': 0, 'dutch': 5, 'english': 9, 'finnish': 0, 'french': 1, 'german': 1, 'greek': 0, 'hebrew': 0, 'hinglish': 12, 'hungarian': 1, 'indonesian': 1, 'italian': 2, 'kazakh': 0, 'nepali': 0, 'norwegian': 0, 'portuguese': 1, 'romanian': 1, 'russian': 0, 'slovene': 0, 'spanish': 1, 'swedish': 0, 'tajik': 0, 'turkish': 0}\n"
     ]
    }
   ],
   "source": [
    "#Manual code\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a dictionary to store the counts of stopwords for each language\n",
    "def classify_language(text):\n",
    "    language_counts = {\n",
    "        'arabic': 0,\n",
    "        'azerbaijani': 0,\n",
    "        'basque': 0,\n",
    "        'bengali': 0,\n",
    "        'catalan': 0,\n",
    "        'chinese': 0,\n",
    "        'danish': 0,\n",
    "        'dutch': 0,\n",
    "        'english': 0,\n",
    "        'finnish': 0,\n",
    "        'french': 0,\n",
    "        'german': 0,\n",
    "        'greek': 0,\n",
    "        'hebrew': 0,\n",
    "        'hinglish': 0,\n",
    "        'hungarian': 0,\n",
    "        'indonesian': 0,\n",
    "        'italian': 0,\n",
    "        'kazakh': 0,\n",
    "        'nepali': 0,\n",
    "        'norwegian': 0,\n",
    "        'portuguese': 0,\n",
    "        'romanian': 0,\n",
    "        'russian': 0,\n",
    "        'slovene': 0,\n",
    "        'spanish': 0,\n",
    "        'swedish': 0,\n",
    "        'tajik': 0,\n",
    "        'turkish': 0\n",
    "    }\n",
    "\n",
    "    # Tokenize the text and remove punctuation\n",
    "    words = nltk.wordpunct_tokenize(text.lower())\n",
    "\n",
    "    # Define stopwords for each language\n",
    "    language_stopwords = {\n",
    "        'arabic': set(stopwords.words('arabic')),\n",
    "        'azerbaijani': set(stopwords.words('azerbaijani')),\n",
    "        'basque': set(stopwords.words('basque')),\n",
    "        'bengali': set(stopwords.words('bengali')),\n",
    "        'catalan': set(stopwords.words('catalan')),\n",
    "        'chinese': set(stopwords.words('chinese')),\n",
    "        'danish': set(stopwords.words('danish')),\n",
    "        'dutch': set(stopwords.words('dutch')),\n",
    "        'english': set(stopwords.words('english')),\n",
    "        'finnish': set(stopwords.words('finnish')),\n",
    "        'french': set(stopwords.words('french')),\n",
    "        'german': set(stopwords.words('german')),\n",
    "        'greek': set(stopwords.words('greek')),\n",
    "        'hebrew': set(stopwords.words('hebrew')),\n",
    "        'hinglish': set(stopwords.words('hinglish')),\n",
    "        'hungarian': set(stopwords.words('hungarian')),\n",
    "        'indonesian': set(stopwords.words('indonesian')),\n",
    "        'italian': set(stopwords.words('italian')),\n",
    "        'kazakh': set(stopwords.words('kazakh')),\n",
    "        'nepali': set(stopwords.words('nepali')),\n",
    "        'norwegian': set(stopwords.words('norwegian')),\n",
    "        'portuguese': set(stopwords.words('portuguese')),\n",
    "        'romanian': set(stopwords.words('romanian')),\n",
    "        'russian': set(stopwords.words('russian')),\n",
    "        'slovene': set(stopwords.words('slovene')),\n",
    "        'spanish': set(stopwords.words('spanish')),\n",
    "        'swedish': set(stopwords.words('swedish')),\n",
    "        'tajik': set(stopwords.words('tajik')),\n",
    "        'turkish': set(stopwords.words('turkish'))\n",
    "    }\n",
    "\n",
    "    # Classify words by language and count stopwords for each language\n",
    "    for word in words:\n",
    "        for language, stopwords_list in language_stopwords.items():\n",
    "            if word in stopwords_list:\n",
    "                language_counts[language] += 1\n",
    "\n",
    "    # Return the language counts\n",
    "    return language_counts\n",
    "\n",
    "# Test text\n",
    "test = \"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\"\n",
    "\n",
    "result = classify_language(test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708fa5",
   "metadata": {
    "id": "43708fa5"
   },
   "source": [
    "# Question 3\n",
    " Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f761e",
   "metadata": {
    "id": "2e1f761e"
   },
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zrory zrory zrory zindagy zindagy sehr bein balo mera zindagy zindagy aram\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def normalize_word(word):\n",
    "    # Apply rules one by one\n",
    "    if word.endswith(\"ain\"):\n",
    "        word = word.replace(\"ain\", \"ein\")\n",
    "    if not (word.startswith(\"ar\")) and \"ar\" in word:\n",
    "        word = word.replace(\"ar\", \"r\")\n",
    "    word = word.replace(\"ai\", \"ae\")\n",
    "    word = word.replace(\"iy\", \"I\")\n",
    "    word = word.replace(\"ih\", \"eh\")\n",
    "    word = word.replace(\"ay\", \"e\")\n",
    "    word = word.replace(\"s\" * 3, \"s\")\n",
    "    word = word.replace(\"ry\", \"ri\")\n",
    "    if word.startswith(\"es\"):\n",
    "        word = \"is\" + word[2:]\n",
    "    word = word.replace(\"sy\", \"si\")\n",
    "    if not word.endswith(\"ty\"):\n",
    "        word = word.replace(\"ty\", \"ti\")\n",
    "    word = word.replace(\"aaa\", \"a\")\n",
    "    word = word.replace(\"aa\", \"a\")\n",
    "    word = word.replace(\"j\" * 2, \"j\")\n",
    "    word = word.replace(\"o\" * 2, \"o\")\n",
    "    if(\"ee\" in word):\n",
    "        word = word.replace(\"ee\", \"i\")\n",
    "    word = word.replace(\"ee\" * 2, \"i\")\n",
    "\n",
    "    if word.endswith(\"i\") and word[-1] not in ['a','s']:\n",
    "        word = word[:-1] + \"y\"\n",
    "    word = word.replace(\"d\" * 2, \"d\")\n",
    "    word = word.replace(\"u\", \"o\")\n",
    "    if word.startswith(\"h\"):\n",
    "        word = word[1:]\n",
    "\n",
    "    return word\n",
    "\n",
    "def normalize_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    normalized_words = [normalize_word(word) for word in words]\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Test the code\n",
    "input_sentence = \"zaroori zaruri zarori zindagee zindagy sehar bain balu mera zindagi zindagi aram\"\n",
    "normalized_sentence = normalize_sentence(input_sentence)\n",
    "print(normalized_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m7iy18K4vJYt",
   "metadata": {
    "id": "m7iy18K4vJYt"
   },
   "source": [
    "# Question 4\n",
    "In this question, you have been given two text files in Urdu. The first file contains an Urdu dictionary,\n",
    "which consists of a list of words. The second file contains sentences that do not have spaces between the\n",
    "words and are difficult to read.\n",
    "آجخودبخشہوں\n",
    "This sentence, without proper word segmentation, is difficult to understand. However, with proper word\n",
    "segmentation, the sentence can be separated into individual words:\n",
    "آج خود بخش ہوں\n",
    "This makes the sentence much easier to read and understand.\n",
    "\n",
    "\n",
    "This task is create spaces between words using\n",
    "\n",
    "*   unigrams\n",
    "*   bigram\n",
    "*   trigrams\n",
    "\n",
    "You can use the list of words file/dictionary provided in assignment 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3007bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File without proper segmentation: \n",
      "\n",
      "{'سابقوفاقیوزیرمفتاحاسماعیلکاکہناہےکہوزیرخزانہاسحاقڈارنےپھروہیحرکتکیجوشوکتتریننےکیتھیانہوںنےآئیایمایفسےکیاگیامعاہدہتوڑاتھاانہوںنےہمنیوزکےپروگراممیںگفتگوکرتےہوئےمزیدکہاکہآئیایمایفسےمعاہدہہونےکےبعدچیزیںبہترہوںگیپیآئیاےاسسال90اربروپےکانقصانکرےگیلیکناگرپیآئیاےکینجکاریہوگیتو90اربروپےکانقصاننہیںہوگا', 'قبلازیںوفاقیوزیرایازصادقنےکہاکہپرویزخٹکاسدقیصراوراعجازشاہکواےپیسیمیںشرکتکیدعوتدیانسےکہاکہوہیہدعوتعمرانخانتکپہنچادیں', 'آفسپنرنےکہاکہانٹرنیشنلکرکٹکونسل(آئیسیسی)نےپاکستانکوٹورنامنٹکیمیزبانیکاحقدےدیاہےلیکنبھارتپاکستانکادورہکرنےکوتیارنہیںرویچندرنایشوننےبھی2023ءمیں50اوورکےورلڈکپکےحوالےسےپاکستانکرکٹبورڈ(پیسیبی)کےحالیہبیانکاجوابدیتےہوئےکہامیرےخیالمیںیہممکننہیںہےآفسپنرنےمزیدکہاکہپاکستاننےپہلےبھارتکادورہکرنےسےانکارکردیاتھالیکنآخرکاروہمیگاایونٹسمیںشرکتکےلیےبھارتگئےتھے', 'مفتاحاسماعیلنےکہاکہمیںتوچاہرہاتھاپہلےدنہیپٹرولیممصنوعاتکیقیمتوںمیںاضافہکیاجائےسیلابآنےسےپہلےہمنےڈیفالٹرسککوکمکیاتھاشہبازشریفاگرآجوزیراعظمنہہوتےتوملکڈیفالٹکیطرفجاچکاتھاقبلازیںایکبیانمیںمفتاحاسماعیلنےکہاکہہمسلملیگمیںکوئیفارورڈبلاکنہیںبنرہاپاکستانڈیفالٹنہیںکرےگاپارٹیکےموجودہنائبصدرکوپارٹیکےفیصلےکرنےکااختیارحاصلہےوہگزشتہروزکراچیمیںنیشنلاسلامکاکنامککانفرنسسےخطابکررہےتھے', 'سابقوزیرخزانہنےاپنےخطاباورمیڈیاسےباتچیتکےدورانمسلملیگمیںفارورڈبلاکبننےکیخبروںکومستردکرتےہوئےکہاکہایسیاطلاعاتمیںکوئیصداقتنہیںہےشاہدخاقانعباسیپہلےہیکہہچکےہیںکہوہپارٹیمیںہیہیںانہوںنےاپنےپارٹیعہدہسےاستعفیٰدیاہےپارٹینہیںچھوڑیہےملککیمعاشیصورتحالکےحوالےسےمفتاحاسماعیلکاکہناتھاکہملککےڈیفالٹہونےکاکوئیامکانہیںآئیایمایفسےڈیلہوجائیگیاورمعاملاتدرستہوجائینگےانکاکہناتھاکہاگرشوکتترینفروریمیںآئیایمایفکامعاہدہنہتوڑتےتوحالاتاچھےہوتےپیٹیآئیکیحکومتمیں80اربڈالرکیامپورٹہوئیڈالرکوروککررکھناغلطیتھیاوراسلئےڈالرکیقیمتآجبلندترینسطحپرہےڈالرکیقدرکوپکڑکرروکانہیںجاسکتاڈالرکافریفلوٹرہناہیبہترہےغیرضروریمشینریکوہمنےامپورٹسےروکاتھاابھیجوپابندیہےوہکچھاورہے', 'اےپیسیمیںتمامجماعتوںکیشرکتکےلیے2دنبڑھادیئےہیںاےپیسیمیںکسیجماعتکےشاملنہہونےسےمعاملہرکنہیںسکتاوزیرداخلہکاکہناتھاکہقومیمعاملےکاتمامسیاسیجماعتوںکوملکرحلنکالناہوگااےپیسیمیںاتفاقرائےکےبعدحکومتدہشتگردوںکےخلافکارروائیکافیصلہکرےگی', 'تفصیلاتکےمطابقسیشنعدالتمیںالیکشنکمیشنکیعمرانخانکےخلاففوجداریمقدمےکیدرخواستپرسماعتہوئیججظفراقبالنےدرخواستپرسماعتکیعمرانخانکیجانبسےطبیبنیادوںپرحاضریسےاستثنیٰکیدرخواستدائرکیگئیعدالتنےوکیلسےاستفسارکیاکہکیامچلکےجمعکروادیئےوکیلگوہرعلیخاننےکہاکہعمرانخانکےضمانتیمچلکےجمعکروادیئےججنےاستفسارکیاکہایسےحاضریسےاستثنٰیکیدرخواستدائرہوتیرہیتوفردجرمکیسےعائدہوگیوکیلعلیبخارینےبتایاکہہمیںمصدقہکاپیاںفراہمنہیںکیگئیں', 'ایازصادقنےپیٹیآئیرہنمااسدقیصرکےبیانپرردِعملدیتےہوئےکہاہےکہاےپیسیمیںشرکتکیلئےکسیکوبھیدعوتنامہنہیںبھجوایافونکیےیاملاقاتکرکےکانفرنسمیںشرکتکیدعوتدیگئیہےسابقاسپیکرقومیاسمبلیاسدقیصررویوںکیباتنہکریںتواچھاہےپیٹیآئینےنلیگپیپلزپارٹیکےساتھایفآئیاےاورنیبکےذریعےکیاکچھنہیںکیاجبکہتحریکانصافکےرہنمااسدقیصرکاکہناتھاکہمجھےحکومتینمائندوںنےفونپراےپیسیمیںشرکتکیدعوتدیکانفرنسکیدعوتدینےکایہبالکلبھیمناسبطریقہنہیںہےاےپیسیمیںدعوتدیناکااحسنطریقہہوتاہےہمسمجھتےہیںکہملکسنگینبحرانوںکاشکارہےاوراسوقتیکجہتیکیضرورتہےحکومتیرویہغیرآئینیہےاوراسماحولمیںیکجہتیکاسوالہیپیدانہیںہوتاحکومتکوسبسےپہلےاپنےرویےمیںبہتریلاناہوگیہمنےاپنےدورمیںدہشتگردیپرقابوپانےکیلئےبہترینحکمتعملیبناکرامنقائمکیا', 'غورطلبہےکہایشیاکپکےمعاملےپرحالہیمیںبحرینمیںہونےوالیایکہنگامیمیٹنگمیںغورکیاگیاجہاںوینیوکےبارےمیںحتمیفیصلہمارچتکموخرکردیاگیابحرینمیںایشینکرکٹکونسل(اےسیسی)کےاجلاسکےبعدبیسیسیآئیکےحکامنےاعلانکیاکہبورڈنےایونٹکےلیےاپنیٹیمپاکستاننہبھیجنےکافیصلہکیاہےتاہمپیسیبیکےحکامنےبھیسختردعملکااظہارکرتےہوئےکہاہےکہوہاکتوبرمیںبھارتمیںہونےوالےورلڈکپ2023ءمیںشرکتنہیںکریںگے', 'تجربہکارہندوستانیآفسپنررویچندرنایشوننےآئندہایشیاءکپ2023ءکیغیریقینیقسمتپراپنیرائےکااظہارکیاہےجوپاکستانمیںہونےجارہاہےاپنےیوٹیوبچینلپرباتکرتےہوئےرویچندرنایشوننےکہاکہاگرپڑوسیملکبھارتایشیاکپ2023ءمیںشرکتکرناچاہتاہےتومقامتبدیلکردیناچاہیے', 'یادرہےکہتوشہخانہکیسمیںعمرانخاننےسینئروکیلخواجہحارثکیخدماتحاصلکرلیںخواجہحارثسمیت4وکلاکےوکالتنامےعدالتمیںجمعکرادیئےگئےہیں', 'وزیرداخلہراناثناءاللہکاکہناہےکہعمرانخاننےجیلبھروتحریککااعلانکیاہےوہپہلےبھیاسہتھکنڈےمیںناکامہوئےعمرانخانکومعلومہینہیںکہجیلمیںرہناکتنامشکلہےمیڈیارپورٹسکےمطابقراناثناءاللہنےاپنےبیانمیںکہاکہعمرانخانکامقصدسیاسیافراتفریہےوہاسمیںناکامہوںگےعمرانخاناپنیزندگیکاصرفایکدنجیلمیںرہے', 'توشہخانہکیسمیںعمرانخانپرفردجرمعائدنہہوسکیسیشنعدالتنےالیکشنکمیشنکیچیئرمینپیٹیآئیکیخلاففوجداریمقدمےکیدرخواستپرعمرانخانکیطبیبنیادوںپرآجحاضریسےاستثنٰیکیدرخواستمنظورکرلیالیکشنکمیشنکیجانبسےپیٹیآئیوکلاکوتصدیقشدہکاپیاںفراہمنہیںکیگئیںتصدیقشدہکاپیاںکیفراہمیکےبعدفردجرمکیاگلیتاریخمقررکیجائےگی', 'ججنےہدایتکیکہتمامثبوتوںکیتصدیقشدہکاپیاںعدالتاورپیٹیآئیکوفراہمکریںوکیلالیکشنکمیشننےکہاکہہمآجہیکمپلینٹاورثبوتوںکیمصدقہکاپیاںفراہمکردیںگےوکیلالیکشنکمیشننےکہاکہعمرانخانابھیتکعدالتکیوںنہیںآئےججنےاستفسارکیاکہکیاکہمجھےایکتاریخبتادیںعمرانخانکبعدالتآئیںگےعمرانخانکےوکیلنےکہاکہعمرانخانکیصحتنےاجازتدیتوآئیںگےڈاکٹرزکیہدایتپرعملکررہےہیںعدالتنےعمرانخانکیآجحاضریسےاستثنیٰپرفیصلہمحفوظکرلیاعدالتنےطبیبنیادوںپرعمرانخانکیآجحاضریسےاستثنیٰکیدرخواستمنظورکرلیچیئرمینپیٹیآئیعمرانخانپرفردجرمعائدنہہوسکی'}\n",
      "\n",
      "File with proper Segmentation: \n",
      "\n",
      "تجربہ کار ہندوستانی آف سپنر روی چندرن ایشون نے آئندہ ایشیاء کپ 2023ء کی غیر یقینی قسمت پر اپنی رائے کا اظہار کیا ہے، جو پاکستان میں ہونے جا رہا ہے۔ اپنے یوٹیوب چینل پر بات کرتے ہوئے روی چندرن ایشون نے کہا کہ اگر پڑوسی ملک بھارت ایشیا کپ 2023ء میں شرکت کرنا چاہتا ہے تو مقام تبدیل کر دینا چاہیے۔\n",
      "آف سپنر نے کہا کہ انٹرنیشنل کرکٹ کونسل (آئی سی سی) نے پاکستان کو ٹورنامنٹ کی میزبانی کا حق دے دیا ہے لیکن بھارت پاکستان کا دورہ کرنے کو تیار نہیں۔ روی چندرن ایشون نے بھی 2023ء میں 50 اوور کے ورلڈ کپ کے حوالے سے پاکستان کرکٹ بورڈ (پی سی بی) کے حالیہ بیان کا جواب دیتے ہوئے کہا، \"میرے خیال میں یہ ممکن نہیں ہے۔\" آف سپنر نے مزید کہا کہ پاکستان نے پہلے بھارت کا دورہ کرنے سے انکار کر دیا تھا لیکن آخر کار وہ میگا ایونٹس میں شرکت کے لیے بھارت گئے تھے۔\n",
      "\n",
      "غور طلب ہے کہ ایشیا کپ کے معاملے پر حال ہی میں بحرین میں ہونے والی ایک ہنگامی میٹنگ میں غور کیا گیا، جہاں وینیو کے بارے میں حتمی فیصلہ مارچ تک موخر کر دیا گیا۔ بحرین میں ایشین کرکٹ کونسل (اے سی سی) کے اجلاس کے بعد، بی سی سی آئی کے حکام نے اعلان کیا کہ بورڈ نے ایونٹ کے لیے اپنی ٹیم پاکستان نہ بھیجنے کا فیصلہ کیا ہے۔ تاہم، پی سی بی کے حکام نے بھی سخت ردعمل کا اظہار کرتے ہوئے کہا ہے کہ وہ اکتوبر میں بھارت میں ہونے والے ورلڈ کپ 2023ء میں شرکت نہیں کریں گے۔\n",
      "\n",
      " توشہ خانہ کیس میں عمران خان پر فرد جرم عائد نہ ہو سکی،سیشن عدالت نے الیکشن کمیشن کی چیئرمین پی ٹی آئی کیخلاف فوجداری مقدمے کی درخواست پر عمران خان کی طبی بنیادوں پر آج حاضری سے استثنٰی کی درخواست منظور کر لی،الیکشن کمیشن کی جانب سے پی ٹی آئی وکلا کو تصدیق شدہ کاپیاں فراہم نہیں کی گئیں تصدیق شدہ کاپیاں کی فراہمی کے بعد فرد جرم کی اگلی تاریخ مقرر کی جائے گی۔\n",
      "تفصیلات کے مطابق سیشن عدالت میں الیکشن کمیشن کی عمران خان کے خلاف فوجداری مقدمے کی درخواست پر سماعت ہوئی،جج ظفر اقبال نے درخواست پر سماعت کی۔ عمران خان کی جانب سے طبی بنیادوں پر حاضری سے استثنیٰ کی درخواست دائر کی گئی۔ عدالت نے وکیل سے استفسار کیا کہ کیا مچلکے جمع کروا دیئے؟ وکیل گوہرعلی خان نے کہا کہ عمران خان کے ضمانتی مچلکے جمع کروا دیئے،جج نے استفسار کیا کہ ایسے حاضری سے استثنٰی کی درخواست دائر ہوتی رہی تو فرد جرم کیسے عائد ہو گی؟ وکیل علی بخاری نے بتایا کہ ہمیں مصدقہ کاپیاں فراہم نہیں کی گئیں۔\n",
      "\n",
      "جج نے ہدایت کی کہ تمام ثبوتوں کی تصدیق شدہ کاپیاں عدالت اور پی ٹی آئی کو فراہم کریں،وکیل الیکشن کمیشن نے کہا کہ ہم آج ہی کمپلینٹ اور ثبوتوں کی مصدقہ کاپیاں فراہم کردیں گے۔ وکیل الیکشن کمیشن نے کہا کہ عمران خان ابھی تک عدالت کیوں نہیں آئے؟ جج نے استفسار کیا کہ کیا کہ مجھے ایک تاریخ بتا دیں عمران خان کب عدالت آئیں گے؟ عمران خان کے وکیل نے کہا کہ عمران خان کی صحت نے اجازت دی تو آئیں گے،ڈاکٹرز کی ہدایت پر عمل کر رہے ہیں،عدالت نے عمران خان کی آج حاضری سے استثنیٰ پر فیصلہ محفوظ کر لیا،عدالت نے طبی بنیادوں پر عمران خان کی آج حاضری سے استثنیٰ کی درخواست منظور کر لی،چیئرمین پی ٹی آئی عمران خان پر فردجرم عائد نہ ہو سکی۔\n",
      "یاد رہے کہ توشہ خانہ کیس میں عمران خان نے سینئر وکیل خواجہ حارث کی خدمات حاصل کر لیں،خواجہ حارث سمیت 4 وکلا کے وکالت نامے عدالت میں جمع کرا دیئے گئے ہیں۔\n",
      "\n",
      "\n",
      "سابق وفاقی وزیر مفتاح اسماعیل کا کہنا ہے کہ وزیر خزانہ اسحاق ڈار نے پھر وہی حرکت کی جو شوکت ترین نے کی تھی،انہوں نے آئی ایم ایف سے کیا گیا معاہدہ توڑا تھا۔انہوں نے ہم نیوز کے پروگرام میں گفتگو کرتے ہوئے مزید کہا کہ آئی ایم ایف سے معاہدہ ہونے کے بعد چیزیں بہتر ہوں گی۔پی آئی اے اس سال 90 ارب روپے کا نقصان کرے گی لیکن اگر پی آئی اے کی نجکاری ہو گی تو 90 ارب روپے کا نقصان نہیں ہوگا۔\n",
      "مفتاح اسماعیل نے کہا کہ میں تو چاہ رہا تھا پہلے دن ہی پٹرولیم مصنوعات کی قیمتوں میں اضافہ کیا جائے۔سیلاب آنے سے پہلے ہم نے ڈیفالٹ رسک کو کم کیا تھا۔شہباز شریف اگر آج وزیراعظم نہ ہوتے تو ملک ڈیفالٹ کی طرف جا چکا تھا۔قبل ازیں ایک بیان میں مفتاح اسماعیل نے کہا کہ ہمسلم لیگ میں کوئی فارورڈ بلاک نہیں بن رہا،پاکستان ڈیفالٹ نہیںکرے گا ،پارٹی کے موجودہ نائب صدر کو پارٹی کے فیصلے کرنے کا اختیار حاصل ہے،وہ گزشتہ روز کراچی میں نیشنل اسلامک اکنامک کانفرنس سے خطاب کررہے تھے۔\n",
      "\n",
      "سابق وزیر خزانہ نے اپنے خطاب اور میڈیا سے بات چیت کے دوران مسلم لیگ میںفارورڈ بلاک بننے کی خبروںکو مسترد کرتے ہوئے کہا کہ ایسی اطلاعات میںکوئی صداقت نہیںہے ،شاہد خاقان عباسی پہلے ہی کہہ چکے ہیںکہ وہ پارٹی میںہی ہیں،انہوںنے اپنے پارٹی عہدہ سے استعفیٰ دیا ہے ،پارٹی نہیں چھوڑی ہے ،ملک کی معاشی صورتحال کے حوالے سے مفتاح اسماعیل کا کہنا تھا کہ ملک کے ڈیفالٹ ہونے کا کوئی امکان ہیں،آئی ایم ایف سے ڈیل ہوجائیگی اور معاملات درست ہوجائینگے،ان کا کہنا تھا کہ اگرشوکت ترین فروری میں آئی ایم ایف کا معاہدہ نہ توڑتے تو حالات اچھے ہوتے،پی ٹی آئی کی حکومت میں 80 ارب ڈالرکی امپورٹ ہوئی،ڈالرکوروک کررکھنا غلطی تھی،اور اس لئے ڈالرکی قیمت آج بلندترین سطح پرہے،ڈالر کی قدر کو پکڑ کر روکا نہیں جاسکتا، ڈالر کا فری فلوٹ رہنا ہی بہتر ہے، غیرضروری مشینری کو ہم نے امپورٹ سے روکا تھا،، ابھی جو پابندی ہے وہ کچھ اور ہے۔\n",
      "\n",
      "وزیر داخلہ رانا ثناء اللہ کا کہنا ہے کہ عمران خا ن نے جیل بھرو تحریک کا اعلان کیا ہے،وہ پہلے بھی اس ہتھکنڈےمیں ناکام ہوئے،عمران خا ن کو معلوم ہی نہیں کہ جیل میں رہنا کتنا مشکل ہے۔ میڈیا رپورٹس کے مطابق رانا ثناء اللہ نے اپنے بیان میں کہا کہ عمران خا ن کا مقصد سیاسی افراتفری ہے وہ اس میں ناکام ہوں گے،عمران خان اپنی زندگی کا صرف ایک دن جیل میں رہے۔\n",
      "اے پی سی میں تمام جماعتوں کی شرکت کے لیے 2 دن بڑھا دیئے ہیں،اے پی سی میں کسی جماعت کے شامل نہ ہونے سے معاملہ رک نہیں سکتا۔ وزیر داخلہ کا کہنا تھا کہ قومی معاملے کا تمام سیاسی جماعتوں کو مل کر حل نکالنا ہو گا،اے پی سی میں اتفاق رائے کے بعد حکومت دہشت گردوں کے خلاف کارروائی کا فیصلہ کرے گی۔\n",
      "\n",
      "قبل ازیں وفاقی وزیر ایاز صادق نے کہا کہ پرویز خٹک،اسد قیصر اور اعجاز شاہ کو اے پی سی میں شرکت کی دعوت دی،ان سے کہا کہ وہ یہ دعوت عمران خان تک پہنچا دیں۔\n",
      "\n",
      "ایاز صادق نے پی ٹی آئی رہنما اسد قیصر کے بیان پر ردِعمل دیتے ہوئے کہا ہے کہ اے پی سی میں شرکت کیلئے کسی کو بھی دعوت نامہ نہیں بھجوایا،فون کیے یا ملاقات کر کے کانفرنس میں شرکت کی دعوت دی گئی ہے۔ سابق اسپیکر قومی اسمبلی اسد قیصر رویوں کی بات نہ کریں تو اچھا ہے،پی ٹی آئی نے ن لیگ،پیپلز پارٹی کے ساتھ ایف آئی اے اور نیب کے ذریعے کیا کچھ نہیں کیا؟ جبکہ تحریک انصاف کے رہنما اسد قیصر کا کہنا تھا کہ مجھے حکومتی نمائندوں نے فون پر اے پی سی میں شرکت کی دعوت دی،کانفرنس کی دعوت دینے کا یہ بالکل بھی مناسب طریقہ نہیں ہے،اے پی سی میں دعوت دینا کا احسن طریقہ ہوتا ہے،ہم سمجھتے ہیں کہ ملک سنگین بحرانوں کا شکار ہے اور اس وقت یکجہتی کی ضرورت ہے،حکومتی رویہ غیر آئینی ہے اور اس ماحول میں یکجہتی کا سوال ہی پیدا نہیں ہوتا،حکومت کو سب سے پہلے اپنے رویے میں بہتری لانا ہو گی،ہم نے اپنے دور میں دہشت گردی پر قابو پانے کیلئے بہترین حکمت عملی بنا کر امن قائم کیا۔\n",
      "Fixing and spacing completed. Result saved in segmented_sentences.txt.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "#  Reading the file containing correct sentences and extract all the words.\n",
    "with open('words_dictionary.txt', 'r', encoding='utf-8') as file:\n",
    "    sentences = file.read().splitlines()\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        words.update(sentence.split())\n",
    "\n",
    "#  Generate unigrams, bigrams, and trigrams from the words.\n",
    "unigrams = words\n",
    "bigrams = set(ngrams(words, 2))\n",
    "trigrams = set(ngrams(words, 3))\n",
    "\n",
    "#  Read the file containing words that don't have spaces.\n",
    "with open('word_test.txt', 'r', encoding='utf-8') as file:\n",
    "    joined_words = set(file.read().split())\n",
    "#Joined words file\n",
    "print(\"File without proper segmentation: \\n\")\n",
    "print(joined_words)\n",
    "print(\"\\nFile with proper Segmentation: \\n\")\n",
    "#  Compare all sentences at once\n",
    "fixed_sentences = []\n",
    "for sentence in sentences:\n",
    "    for trigram in trigrams:\n",
    "        joined_trigram = \" \".join(trigram)\n",
    "        if joined_trigram in joined_words:\n",
    "            sentence = sentence.replace(joined_trigram, \" \".join(trigram))\n",
    "    for bigram in bigrams:\n",
    "        joined_bigram = \" \".join(bigram)\n",
    "        if joined_bigram in joined_words:\n",
    "            sentence = sentence.replace(joined_bigram, \" \".join(bigram))\n",
    "    \n",
    "    for unigram in unigrams:\n",
    "        if unigram in joined_words:\n",
    "            sentence = sentence.replace(unigram, unigram + \" \")\n",
    "    \n",
    "        #fixed_sentences.append(sentence)\n",
    "\n",
    "# : Replace the joined words in the text file with properly spaced words.\n",
    "with open('output_file.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for sentence in fixed_sentences:\n",
    "        output_file.write(sentence + '\\n')\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "\n",
    "print(\"Fixing and spacing completed. Result saved in segmented_sentences.txt.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
